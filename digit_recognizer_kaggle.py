# -*- coding: utf-8 -*-
"""Digit_recognizer_kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oKgKnlatiEsgstTJaplzfNI_CgTENPKP

# **Digit Recognizer**

**Competition Description**

MNIST ("Modified National Institute of Standards and Technology") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.

In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images
"""

from IPython.display import Image
Image(url= "https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.mm.bing.net%2Fth%3Fid%3DOIP.rmWbidEzubGp_bFVpQpjoQHaEV%26pid%3DApi&f=1")

"""## **Collecting the Data**

training data set and testing data set are given by Kaggle you can download from kaggle directly

link-https://www.kaggle.com/c/digit-recognizer/data

## load train, test dataset

The Python Pandas packages helps us work with our datasets. We start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.
"""

pip install mnist

import mnist

train_images=mnist.train_images()
train_labels=mnist.train_labels()
test_images=mnist.test_images()
test_labels=mnist.test_labels()

import pandas as pd
test_kaggle=pd.read_csv('test.csv')

"""## Exploritory Data analysis"""

train_images=train_images/255
test_images=test_images/255

"""We are scaling down the images between 0 and 1

Y divide by 255

Because that is the highest value of a pixel in this dataset
"""

train_images=train_images.reshape((-1,784))
test_images=test_images.reshape((-1,784))

"""Now we reshape the pixels to an 1d array so that we can feed it to neural network"""

train_images.shape

test_images.shape

"""## Data Visulization

Seaborn and Matplot lib is an excellent library used fro data visulization in python
"""

import seaborn as sns
sns.countplot(train_labels)

"""**We can see that the distribution of the data is uniform**"""

train_kaggle=pd.read_csv('train.csv')
train_kaggle=train_kaggle.drop(['label'],axis=1)
train_kaggle=train_kaggle.values.reshape(train_kaggle.shape[0], 28, 28)
train_kaggle=train_kaggle/255

"""**We will plot some random sample data**"""

from matplotlib import pyplot as plt
plt.imshow(train_kaggle[1])

plt.imshow(train_kaggle[23])

plt.imshow(train_kaggle[1324])

plt.imshow(train_kaggle[2132])

"""## Model Creation and Training

**We are creating an Artificial Neural Network in order to categorize the images into ten categories**

We are using Keras library to build a neural network
"""

from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Embedding, Flatten, LeakyReLU, BatchNormalization, Dropout
from keras.activations import relu, sigmoid

model=keras.Sequential()

# Adding the input layer and the first hidden layer
model.add(Dense(128,activation='relu',kernel_initializer = 'he_uniform',input_dim=784))

# Adding the second hidden layer
model.add(Dense(units = 64, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
model.add(Dense(units= 10, kernel_initializer = 'glorot_uniform', activation = 'softmax'))

model.summary()

# Compiling the ANN
model.compile(optimizer = 'Adamax', loss = 'categorical_crossentropy', metrics = ['accuracy'])

from keras.utils import to_categorical
# Fitting the ANN to the Training set
model_history=model.fit(train_images,
                             to_categorical(train_labels)
                            ,batch_size = 10, 
                              nb_epoch = 10)

# summarize history for loss
plt.plot(model_history.history['loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""### Hyper parameter Tuning

We will perform some Hyperparameter optimiztion in order to get the best hyperparamteres which will provide us with  best results

We are using **Randomized search cv** provided by scikit learn to perform hyperparameter optimization
"""

def create_model(layers, activation):
    model = Sequential()
    for i, nodes in enumerate(layers):
        if i==0:
            model.add(Dense(nodes,input_dim=784))
            model.add(Activation(activation))
            model.add(Dropout(0.1))
        else:
            model.add(Dense(nodes))
            model.add(Activation(activation))
            model.add(Dropout(0.1))
            
    model.add(Dense(units = 10, kernel_initializer= 'glorot_uniform', activation = 'softmax')) # Note: no activation beyond this point
    
    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
    return model

model = KerasClassifier(build_fn=create_model)

layers = [ (128, 64),(64,64),(64,32,16)]
activations = [ 'relu']
param_grid = dict(layers=layers, activation=activations, batch_size = [10], epochs=[10])

grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,cv=5)

grid_result = grid.fit(train_images,  to_categorical(train_labels))

print("Best Score:",grid_result.best_score_)
print("-------------------------------------------------------------")
print("Best Parameters selected:",grid_result.best_params_)

"""**So we have got the best hyper paramters that we will use to build our final model**"""

model=keras.Sequential()
# Adding the input layer and the first hidden layer
model.add(Dense(128,activation='relu',kernel_initializer = 'he_uniform',input_dim=784))

# Adding the second hidden layer
model.add(Dense(units = 64, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
model.add(Dense(units= 10, kernel_initializer = 'glorot_uniform', activation = 'softmax'))

# Compiling the ANN
model.compile(optimizer = 'Adamax', loss = 'categorical_crossentropy', metrics = ['accuracy'])

from keras.utils import to_categorical
# Fitting the ANN to the Training set
model_history=model.fit(train_images,
                             to_categorical(train_labels)
                            ,batch_size = 10, 
                              epochs = 20)

"""We have got an accuracy of **97.19** on our training dataset"""

# summarize history for loss
from matplotlib import pyplot as plt
plt.plot(model_history.history['loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""We will check for the acuraccy on testing dataset"""

model.evaluate(test_images,to_categorical(test_labels))

"""### Prediction"""

ans=model.predict(test_kaggle)

import numpy as np
ans=np.argmax(ans,axis=1)

ans[:5]

predicted_classes = model.predict_classes(test_kaggle)
submissions=pd.DataFrame({"ImageId": list(range(1,len(predicted_classes)+1)),
                         "Label": predicted_classes})
submissions.to_csv("subbmision2.csv", index=False, header=True)

ls

